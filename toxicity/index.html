<html>

<head>
    
  <title>Sentence toxicity</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <script src="js/tfjs.js"></script>
    <script src="js/toxicity.js"></script>
    <link href="https://fonts.googleapis.com/css?family=Roboto|Roboto+Mono&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="css/main.css"> 

</head>

<!-- This is a demo of the TensorFlow.js toxicity model, which classifies text according to
            whether it exhibits offensive attributes (i.e. profanity, sexual explicitness). The samples in the table
            below were taken from this -->

<body>
    <div id='main'>
        <h1>Sentence toxicity</h1>
        <div class="description">
        </div>
        <center>
            <div id="loader">
                <div class="lds-ring">
                    <div></div>
                    <div></div>
                    <div></div>
                    <div></div>
                </div>
                <br>
                <br>
                <br>Please wait while the model loads.
            </div>
        </center>
        <div id="table-wrapper"></div>


        <div id="inp">
            <input id="classify-new-text-input" placeholder="enter your text ">
            <div id="classify-new-text">Check</div>
        </div>
    </div>

</body>
<script src="index.js"></script>

</html>
